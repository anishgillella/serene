# Technical Challenges & Solutions

A comprehensive log of all technical challenges encountered during the development of Serene, including context, debugging process, and solutions implemented.

---

##  Voice Agent Challenges

### 1. **LiveKit Agent Not Joining Rooms**

**Problem:**  
Agent entrypoint was being called but Luna wouldn't join the room. No audio, no greetings, silent failure.

**Root Cause:**  
- Incorrect agent dispatch pattern - tried explicit `CreateAgentDispatchRequest` which doesn't work for local dev
- Local agents use `AgentServer` pattern and auto-join based on room name filtering

**Solution:**
- Removed explicit dispatch code
- Implemented room name filtering (`mediator-{conflict_id}`)
- Used `@server.rtc_session()` decorator for automatic room joining
- LocalAgents auto-connect when room matches pattern

**Code:**
```python
@server.rtc_session()
async def mediator_entrypoint(ctx: JobContext):
    room_name = ctx.room.name
    if not room_name.startswith("mediator-"):
        return  # Filter non-mediator rooms
    # Agent automatically joins
```

---

### 2. **Audio Not Playing (Silent Agent)**

**Problem:**  
Agent joined room, processed requests, but no audio output. Logs showed TTS generation but user heard nothing.

**Root Causes:**
1. Missing `ELEVEN_API_KEY` environment variable (plugin checks this specific name)
2. ElevenLabs API key passed as kwarg but plugin expected env var
3. TTS instance created before environment was set

**Solution:**
```python
# Set early in module, before any imports
if os.getenv("ELEVENLABS_API_KEY"):
    os.environ["ELEVEN_API_KEY"] = os.getenv("ELEVENLABS_API_KEY")

# Let plugin read from environment
tts_instance = elevenlabs.TTS(
    model="eleven_flash_v2_5",
    voice_id="21m00Tcm4TlvDq8ikWAM",
    # No api_key kwarg - plugin reads ELEVEN_API_KEY
    streaming_latency=3,
)
```

**Lesson:** LiveKit plugins have specific environment variable expectations - check plugin source code!

---

### 3. **TypeError: chat() Unexpected Keyword Argument**

**Problem:**  
```
TypeError: LLM.chat() got an unexpected keyword argument 'chat_ctx'
```

**Root Cause:**  
LiveKit's LLM interface changed - `chat()` signature expects `chat_ctx` as keyword arg, not positional.

**Solution:**
```python
#  Old way (positional)
stream = llm_instance.chat(chat_ctx, model="...", temperature=0.7)

#  New way (keyword)
stream = llm_instance.chat(chat_ctx=chat_ctx, model="...", temperature=0.7)
```

---

### 4. **STT Endpointing Error**

**Problem:**  
```
TypeError: STT.__init__() got an unexpected keyword argument 'endpointing'
```

**Root Cause:**  
Deepgram plugin doesn't support `endpointing` parameter in newer versions.

**Solution:**  
Remove `endpointing` parameter from STT initialization:
```python
#  Before
stt=deepgram.STT(model="nova-3", endpointing=300)

#  After
stt=deepgram.STT(
    model="nova-3",
    language="en",
    smart_format=True,
    punctuate=True,
    interim_results=True,
)
```

---

### 5. **Double Voice / Agent Duplication**

**Problem:**  
Two Luna voices responding simultaneously, creating echo and confusion.

**Root Cause:**  
Both explicit dispatch AND auto-join were active, creating two agent instances in the same room.

**Solution:**  
Disabled explicit dispatch for local development:
```python
# Commented out explicit dispatch
# req = api.CreateAgentDispatchRequest(room=room_name)
# await lkapi.agent_dispatch.create_dispatch(req)
```

---

### 6. **Agent Startup Latency (12+ seconds)**

**Problem:**  
Luna took 12+ seconds to greet after user joined room.

**Root Causes:**
1. Sequential RAG queries (transcript + calendar + conflict memory)
2. Slow calendar service queries (5+ seconds)
3. No parallel execution

**Solution:**  
Implemented parallel context fetching with `asyncio.gather`:
```python
async def fetch_transcript_context(): ...
async def fetch_calendar_context(): ...

results = await asyncio.gather(
    fetch_transcript_context(),
    fetch_calendar_context(),
    return_exceptions=True
)

transcript_context = results[0]
calendar_context = results[1]
```

**Improvement:** 12s ‚Üí 4s startup time (67% faster)

---

### 7. **VAD Detection Too Sensitive**

**Problem:**  
Agent interrupted user mid-sentence, thinking they stopped speaking.

**Solution:**  
Tuned Silero VAD parameters:
```python
vad=silero.VAD.load(
    min_speech_duration=0.1,   # Shorter to catch quick speech
    min_silence_duration=0.3,  # Longer to avoid false stops
)
```

---

##  RAG System Challenges

### 8. **Pinecone Connection Timeouts**

**Problem:**  
Random `TimeoutError` when querying Pinecone, especially during peak usage.

**Root Cause:**  
No timeout handling, queries would hang indefinitely.

**Solution:**  
Added timeouts to all Pinecone queries:
```python
try:
    results = await asyncio.wait_for(
        asyncio.to_thread(pinecone_service.query, ...),
        timeout=3.0
    )
except asyncio.TimeoutError:
    logger.warning("Pinecone query timed out")
    return []  # Graceful degradation
```

---

### 9. **Context Window Overflow**

**Problem:**  
Full transcript + profiles + past conflicts exceeded GPT-4o-mini's 128k context window.

**Solution:**
1. Implemented reranking to reduce chunks from 20 ‚Üí 7
2. Limited transcript to first 15 chunks for greeting
3. Used structured context formatting with clear sections

**Code:**
```python
# Before: Send all 20 chunks (6k tokens)
chunks = pinecone_results[:20]

# After: Rerank and take top 7 (2.5k tokens)
reranked = reranker_service.rerank(query, chunks, top_k=7)
```

---

### 10. **Hallucinations About Past Conversations**

**Problem:**  
Luna would reference conversations that never happened or misattribute quotes.

**Root Causes:**
1. No reranking - irrelevant chunks mixed with relevant ones
2. Weak semantic boundaries between different conflicts
3. Missing conflict_id filtering

**Solutions:**
1. Added Voyage-Rerank-2 for precision
2. Explicit metadata filtering by `conflict_id`
3. Clear context headers separating sections

**Code:**
```python
# Filter by conflict_id first
results = pinecone_service.index.query(
    vector=query_embedding,
    top_k=20,
    filter={"conflict_id": {"$eq": conflict_id}},  # ‚Üê Critical
    namespace="transcript_chunks",
)

# Then rerank for relevance
reranked = reranker_service.rerank(query, results, top_k=7)
```

---

### 11. **Chunk Order Destroyed Conversation Flow**

**Problem:**  
Vector search returned chunks out of order, breaking narrative coherence.

**Root Cause:**  
Semantic similarity doesn't preserve temporal order.

**Solution:**  
Sort by `chunk_index` after retrieval:
```python
chunks_sorted = sorted(
    chunks,
    key=lambda c: c.metadata.get("chunk_index", 0)
)
```

---

### 12. **Profile Retrieval Missing Context**

**Problem:**  
Luna couldn't access partner profiles even though they were uploaded to Pinecone.

**Root Cause:**  
Wrong namespace - profiles stored in `profiles` namespace but queried from `transcript_chunks`.

**Solution:**  
Separate queries per namespace:
```python
# Transcripts
transcript_results = pinecone_service.query(
    ..., namespace="transcript_chunks"
)

# Profiles
profile_results = pinecone_service.query(
    ..., namespace="profiles"
)
```

---

## üóÑ Database & Storage Challenges

### 13. **Supabase RLS Blocking Direct Access**

**Problem:**  
Backend couldn't read conflicts even with service role key - 401 Forbidden errors.

**Root Cause:**  
Row Level Security (RLS) policies blocked non-authenticated requests even from backend.

**Solution:**  
Created `db_service.py` for direct PostgreSQL access, bypassing Supabase client:
```python
def get_connection(self):
    return psycopg2.connect(
        settings.DATABASE_URL,  # Direct connection string
        connect_timeout=5
    )
```

---

### 14. **Database Connection Pool Exhaustion**

**Problem:**  
```
FATAL: remaining connection slots are reserved for non-replication superuser connections
```

**Root Cause:**  
Connections not closed after use, leaked until pool exhausted.

**Solution:**  
Implemented context manager for automatic cleanup:
```python
@contextmanager
def get_db_context(self):
    conn = None
    try:
        conn = self.get_connection()
        yield conn
    finally:
        if conn:
            conn.close()  # Always closes
```

---

### 15. **"Connection Already Closed" Errors**

**Problem:**  
Random `psycopg2.InterfaceError: connection already closed` during operations.

**Root Cause:**  
Trying to reuse connections across async boundaries.

**Solution:**  
Each operation gets fresh connection via context manager:
```python
def get_conflict(self, conflict_id: str):
    with self.get_db_context() as conn:  # Fresh connection
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("SELECT * FROM conflicts WHERE id = %s", (conflict_id,))
            return cursor.fetchone()
    # Connection auto-closed
```

---

### 16. **S3 Upload Failures (Silent)**

**Problem:**  
Files uploaded to S3 but returned `None` instead of URL, breaking downstream logic.

**Root Cause:**  
Missing error handling - boto3 silently failed without exceptions.

**Solution:**  
Added explicit error handling and logging:
```python
def upload_file(self, file_path, file_content, content_type):
    try:
        self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=file_path,
            Body=file_content,
            ContentType=content_type
        )
        url = f"s3://{self.bucket_name}/{file_path}"
        logger.info(f" Uploaded to S3: {url}")
        return url
    except Exception as e:
        logger.error(f" S3 upload failed: {e}")
        return None  # Explicit failure
```

---

### 17. **JSONB Array Concatenation Bug**

**Problem:**  
Mediator messages not appending - each save overwrote previous messages.

**Root Cause:**  
Wrong PostgreSQL operator - `||` concatenates but doesn't handle arrays correctly.

**Solution:**  
Fixed JSONB array concatenation:
```sql
--  Before
content = mediator_messages.content || %s::jsonb

--  After
content = mediator_messages.content || %s::jsonb
-- Works correctly with proper JSONB array casting
```

---

##  Calendar & Cycle Tracking Challenges

### 18. **Calendar Service Hanging (5+ second queries)**

**Problem:**  
Calendar insights took 5-8 seconds to load, blocking agent startup.

**Root Cause:**  
Complex cycle prediction algorithm running synchronously on every request.

**Solutions:**
1. Implemented 5-minute in-memory cache:
```python
@lru_cache(maxsize=128)
def get_calendar_insights_for_llm(relationship_id: str):
    # Cache key includes timestamp rounded to 5-min intervals
    cache_key = f"{relationship_id}_{int(time.time() // 300)}"
    ...
```

2. Added 3-second timeout:
```python
calendar_context = await asyncio.wait_for(
    asyncio.to_thread(calendar_service.get_calendar_insights_for_llm, ...),
    timeout=3.0
)
```

**Improvement:** 5-8s ‚Üí 0.1s (cached) or 3s max (timeout)

---

### 19. **Cycle Phase Calculation Errors**

**Problem:**  
Wrong cycle phase displayed - "Ovulation" when user was actually in "Luteal" phase.

**Root Cause:**  
Off-by-one error in day calculation from last period start.

**Solution:**
```python
#  Correct calculation
days_since_period = (today - last_period_date).days

# Phase logic
if days_since_period < 0:
    return "Unknown"
elif days_since_period <= 7:
    return "Menstrual"
elif days_since_period <= 13:
    return "Follicular"
elif days_since_period <= 16:
    return "Ovulation"
else:
    return "Luteal"
```

---

##  Frontend Challenges

### 20. **404 Errors on History & Fight Capture Pages**

**Problem:**  
Vercel deployment showed 404 for `/history` and `/fight-capture` routes.

**Root Cause:**  
Missing `vercel.json` rewrite rules for SPA routing.

**Solution:**
```json
{
  "rewrites": [
    { "source": "/(.*)", "destination": "/index.html" }
  ]
}
```

---

### 21. **LiveKit Room Connection Failures**

**Problem:**  
Frontend couldn't connect to LiveKit room - "Invalid token" errors.

**Root Cause:**  
Token generation included incorrect room name or missing permissions.

**Solution:**  
Ensured token matches room pattern:
```typescript
const roomName = `mediator-${conflictId}`;
const response = await fetch(`${API_BASE}/api/mediator/token`, {
  method: 'POST',
  body: JSON.stringify({ conflict_id: conflictId })
});
```

---

### 22. **Speaking Animation Not Updating**

**Problem:**  
Luna's "speaking" animation stuck in wrong state - showed speaking when silent.

**Root Cause:**  
LiveKit track subscription events not properly clearing state.

**Solution:**
```typescript
useEffect(() => {
  const handleTrackSubscribed = (track: RemoteTrack) => {
    if (track.kind === 'audio' && track.source === Track.Source.Microphone) {
      setSpeaking(true);
    }
  };
  
  const handleTrackUnsubscribed = () => {
    setSpeaking(false);  // ‚Üê Always clear on unsubscribe
  };
  
  room.on(RoomEvent.TrackSubscribed, handleTrackSubscribed);
  room.on(RoomEvent.TrackUnsubscribed, handleTrackUnsubscribed);
  
  return () => {
    room.off(RoomEvent.TrackSubscribed, handleTrackSubscribed);
    room.off(RoomEvent.TrackUnsubscribed, handleTrackUnsubscribed);
  };
}, [room]);
```

---

### 23. **Transcript Not Displaying After Recording**

**Problem:**  
User stopped recording but transcript showed "No messages" despite successful storage.

**Root Cause:**  
Frontend used Supabase client (blocked by RLS) instead of backend API.

**Solution:**  
Fetch via backend API which uses direct PostgreSQL:
```typescript
const response = await fetch(`${API_BASE}/api/conflicts/${conflictId}`, {
  headers: { 'ngrok-skip-browser-warning': 'true' }
});
const data = await response.json();
setTranscript(data.transcript);  // From backend, not Supabase
```

---

##  LLM & Prompt Engineering Challenges

### 24. **Luna's Repetitive Language**

**Problem:**  
Luna overused "man", "bro", "dude" in every response, sounding robotic.

**Root Cause:**  
Overly prescriptive system prompt with example phrases.

**Solution:**  
Updated prompt to encourage variety:
```python
DEFAULT_INSTRUCTIONS = """
Your personality:
- Talk like a friend, not a therapist
- Keep it real and casual (2-3 sentences max for voice)
- Vary your language naturally - don't overuse "man", "bro", or "dude"
- Mix casual phrases like "I hear you", "That's tough", "I get it"
- Be warm and empathetic, but conversational
"""
```

---

### 25. **Conflict Analysis Missing Key Details**

**Problem:**  
Generated analysis was generic, missing specific quotes or context from transcript.

**Root Cause:**  
Prompt didn't emphasize using actual transcript content.

**Solution:**  
Enhanced prompt with explicit instructions:
```python
prompt = f"""
Analyze this conflict transcript. Use SPECIFIC QUOTES from the conversation.

IMPORTANT:
- Reference actual statements made (e.g., "When Elara said '...'")
- Cite specific moments, not generalizations
- Ground analysis in observable behavior from transcript

Transcript:
{transcript_text}
"""
```

---

### 26. **Repair Plans Too Generic**

**Problem:**  
Repair plans felt like templates, not personalized to the couple.

**Root Cause:**  
No partner profile context included in prompt.

**Solution:**  
Injected profile information:
```python
context = f"""
Partner Profiles:
{boyfriend_profile}
{girlfriend_profile}

Transcript:
{transcript_text}

Generate a repair plan that considers:
- {partner_names['A']} personality and communication style
- {partner_names['B']} values and sensitivities
- Specific issues raised in THIS conversation
"""
```

---

##  Performance Challenges

### 27. **Background Task Generation Slow (30+ seconds)**

**Problem:**  
Analysis + Repair Plan generation took 30+ seconds, blocking UI.

**Root Cause:**  
Sequential LLM calls (Analysis ‚Üí Repair Plan A ‚Üí Repair Plan B).

**Solution:**  
Parallelized ALL LLM calls using `asyncio.gather`:
```python
analysis, repair_plan_boyfriend, repair_plan_girlfriend = await asyncio.gather(
    analyze_conflict_transcript(...),
    generate_repair_plan(partner_id="partner_a", ...),
    generate_repair_plan(partner_id="partner_b", ...)
)
```

**Improvement:** 30s ‚Üí 12s (60% faster)

---

### 28. **Embedding API Rate Limits**

**Problem:**  
```
voyageai.error.RateLimitError: 429 Too Many Requests
```

**Root Cause:**  
Batch embedding 50+ chunks exceeded Voyage AI rate limits.

**Solution:**  
Added exponential backoff retry:
```python
def embed_batch(texts, max_retries=3):
    for attempt in range(max_retries):
        try:
            return voyage_client.embed(texts, model="voyage-3")
        except RateLimitError:
            wait = 2 ** attempt  # 1s, 2s, 4s
            logger.warning(f"Rate limited, waiting {wait}s...")
            time.sleep(wait)
    raise Exception("Max retries exceeded")
```

---

##  Debugging & Development Challenges

### 29. **Silent Failures in Background Tasks**

**Problem:**  
Background tasks crashed but FastAPI didn't log errors - debugging was impossible.

**Solution:**  
Added comprehensive try-catch with logging:
```python
async def generate_all_background(...):
    try:
        # ... task logic
    except Exception as e:
        logger.error(f" Background task failed: {e}")
        import traceback
        logger.error(traceback.format_exc())  # Full stack trace
```

---

### 30. **Docker Compose Port Conflicts**

**Problem:**  
`docker-compose up` failed with "port already in use" for Neo4j.

**Root Cause:**  
Abandoned Neo4j implementation left running containers.

**Solution:**
```bash
# Clean up all containers
docker-compose down -v

# Eventually removed Neo4j entirely
rm docker-compose.yml
```

---

### 31. **Environment Variables Not Loading**

**Problem:**  
`KeyError` for API keys even though they were in `.env` file.

**Root Cause:**  
Multiple `.env` files (`.env`, `.env.local`) with conflicting priorities.

**Solution:**  
Load both with correct precedence:
```python
load_dotenv(".env.local")  # Load first (lower priority)
load_dotenv(".env")        # Load second (overrides)
```

---

##  Architecture Decisions & Trade-offs

### 32. **Removed Neo4j Graph Database**

**Decision:** Removed Neo4j entirely after initially planning a graph-based relationship memory.

**Reasoning:**
- Over-engineering for MVP
- Vector search with metadata filtering sufficient for pattern recognition
- Neo4j added deployment complexity (Docker requirement)
- Pinecone namespace filtering provided similar capability

**Trade-off:** Lost explicit "saga" tracking but gained simplicity and deployment ease.

---

### 33. **Removed Girlfriend Repair Plans**

**Decision:** Only generate repair plan for boyfriend (Partner A), not girlfriend.

**Reasoning:**
- MVP focused on single-user experience (Adrian's perspective)
- Halved LLM costs
- Reduced latency by 33%
- Can add multi-perspective later if needed

**Trade-off:** Less comprehensive but faster and cheaper.

---

### 34. **Summary-First RAG vs Full Transcript**

**Decision:** Use conflict analysis summary (300 tokens) instead of full transcript (5000+ tokens) for Luna's primary context.

**Reasoning:**
- 94% token reduction
- Faster response times
- Forces focus on core issues vs tangential details
- LLM less prone to getting lost in minutiae

**Trade-off:** Lost verbatim quote ability, but gained clarity and speed.

---

##  Book Reference RAG Challenges

### 35. **Book RAG Retrieving Irrelevant Content**

**Problem:**  
RAG system was retrieving copyright pages, acknowledgments, and other "junk" pages instead of actual book content when queried.

**Root Cause:**  
No filtering logic during book ingestion - all pages were indexed equally, including non-content pages.

**Solution:**  
Implemented heuristic-based filtering to skip junk pages:
```python
def is_likely_junk(text: str) -> bool:
    junk_indicators = [
        "copyright ¬©",
        "all rights reserved",
        "acknowledgments",
        "dedication",
        "table of contents",
        "index\n",
        # ... more patterns
    ]
    text_lower = text.lower()
    return any(indicator in text_lower for indicator in junk_indicators)

# During ingestion
if is_likely_junk(chunk_text):
    logger.info(f" Skipping junk chunk: {chunk_text[:50]}...")
    continue
```

**Impact:** Clean, relevant book references in RAG results

---

### 36. **Pinecone Namespace Cleanup**

**Problem:**  
Previously indexed "junk" book chunks remained in Pinecone even after fixing ingestion logic, polluting search results.

**Solution:**  
Created cleanup script to completely wipe `books` namespace:
```python
# backend/scripts/clear_books.py
from app.services.pinecone_service import pinecone_service

def clear_books_namespace():
    index = pinecone_service.index
    index.delete(namespace="books", delete_all=True)
    logger.info(" Cleared all vectors from 'books' namespace")
```

**Lesson:** Always plan for data cleanup when iterating on RAG ingestion logic.

---

### 37. **SQL Database Orphaned Records**

**Problem:**  
After deleting book from Pinecone, the SQL record still showed in UI, causing confusion.

**Root Cause:**  
No corresponding database cleanup - only Pinecone vectors were deleted.

**Solution:**  
Added `delete_profile` method to `db_service.py`:
```python
def delete_profile(self, pdf_id: str) -> bool:
    try:
        with self.get_db_context() as conn:
            with conn.cursor() as cursor:
                cursor.execute(
                    "DELETE FROM relationship_profiles WHERE pdf_id = %s",
                    (pdf_id,)
                )
                conn.commit()
                return cursor.rowcount > 0
    except Exception as e:
        logger.error(f"Failed to delete profile: {e}")
        return False
```

Created script `delete_book_record.py` to clean up specific records.

---

### 38. **TypeError in update_profile After Refactor**

**Problem:**  
```python
TypeError: update_profile() got an unexpected keyword argument 'profile_id'
```

**Root Cause:**  
`DatabaseService.update_profile()` method was refactored to accept `pdf_id` and an `updates` dictionary, but call site in `pdf_upload.py` still used old signature with `profile_id` and individual keyword arguments.

**Solution:**  
Updated call site to match new signature:
```python
#  Before
db_service.update_profile(
    profile_id=pdf_id,
    extracted_text_length=len(extracted_text),
    file_path=s3_url
)

#  After
db_service.update_profile(
    pdf_id=pdf_id,
    updates={
        "extracted_text_length": len(extracted_text),
        "file_path": s3_url
    }
)
```

**Lesson:** Update all call sites when refactoring method signatures - search codebase for method name.

---

##  Upload UI Challenges

### 39. **Upload Page Log Overflow**

**Problem:**  
Upload logs leaked out of their container horizontally on the Upload page, breaking layout.

**Root Cause:**  
CSS used `whitespace-nowrap` which prevented text wrapping, causing horizontal overflow.

**Solution:**  
Changed CSS to allow text wrapping:
```tsx
//  Before
<div className="whitespace-nowrap">{log}</div>

//  After
<div className="whitespace-pre-wrap break-words border-b border-gray-800/50 last:border-0 py-0.5">
  {cleanLog}
</div>
```

---

### 40. **Upload Log Timestamps Cluttering Display**

**Problem:**  
Backend logs included timestamps (`HH:MM:SS - message`) which cluttered the minimal, clean UI aesthetic.

**Solution:**  
Strip timestamps on frontend before display:
```tsx
{file.logs.map((log, i) => {
  // Remove timestamp (HH:MM:SS - )
  const cleanLog = log.replace(/^\d{2}:\d{2}:\d{2} - /, '');
  return <div key={i}>{cleanLog}</div>;
})}
```

---

### 41. **Upload Completion Not Detected**

**Problem:**  
UI showed perpetual "processing" spinner even after upload completed successfully.

**Root Cause:**  
Frontend checked for log message "Processing complete!" but backend logged "Updated database record" as final success message.

**Solution:**  
Updated frontend completion check to recognize both messages:
```tsx
// Check for completion or error
if (lastLog.includes("Processing complete!") || lastLog.includes("Updated database record")) {
  clearInterval(interval);
  fetchExistingFiles(); // Refresh list
  return { ...f, logs: data.logs, status: 'success', progress: 100 };
}
```

---

### 42. **Duplicate Empty Profile in Upload List**

**Problem:**  
After re-upload attempt, UI showed duplicate "Boyfriend Profile" entry with 0 chars extracted.

**Root Cause:**  
Failed upload created database record but didn't populate text, leaving orphaned record.

**Solution:**  
Created cleanup script to remove empty profiles:
```python
# backend/scripts/delete_empty_profile.py
profiles = db_service.get_profiles(relationship_id=RELATIONSHIP_ID)
for p in profiles:
    if p.get("pdf_type") == "boyfriend_profile" and p.get("extracted_text_length", 0) == 0:
        db_service.delete_profile(p["pdf_id"])
        logger.info(f" Deleted empty profile: {p.get('filename')}")
```

---

##  Voice Agent Tuning Challenges

### 43. **VAD Interruption Not Working**

**Problem:**  
Agent didn't stop speaking when user tried to interrupt, continuing to talk over them.

**Root Cause:**  
VAD (Voice Activity Detection) settings too conservative - `min_speech_duration=0.1s` and `min_silence_duration=0.3s` were too high for responsive interruption.

**Attempted Solution:**  
Tuned VAD parameters for faster detection:
```python
vad=silero.VAD.load(
    min_speech_duration=0.05,  # 100ms ‚Üí 50ms
    min_silence_duration=0.25,  # 300ms ‚Üí 250ms
)
```

Additionally enabled explicit interruptions:
```python
super().__init__(
    instructions=instructions,
    tools=tools or [],
    allow_interruptions=True  # ‚Üê Explicit flag
)
```

**Outcome:** User reported improvement but ultimately reverted changes per their request to keep codebase clean.

---

##  ElevenLabs TTS Challenges

### 44. **ElevenLabs WebSocket Closed Unexpectedly**

**Problem:**  
```
WARNING - websocket closed unexpectedly
APIStatusError: connection closed (status_code=-1, request_id=None, body=None, retryable=True)
```

Agent would connect but TTS failed immediately, preventing voice output.

**Root Cause:**  
After `git reset --hard`, code comment indicated "don't pass api_key - let it use ELEVEN_API_KEY from environment", but the plugin wasn't picking up the environment variable correctly.

**Solution:**  
Explicitly pass `api_key` to TTS constructor:
```python
#  Before (after reset)
tts_instance = elevenlabs.TTS(
    model="eleven_flash_v2_5",
    voice_id="5ztkbGZ95SpVJ8MBMeam",
    # Don't pass api_key - let it use ELEVEN_API_KEY from environment
    streaming_latency=3,
)

#  After
tts_instance = elevenlabs.TTS(
    model="eleven_flash_v2_5",
    voice_id="5ztkbGZ95SpVJ8MBMeam",
    api_key=elevenlabs_key,  # ‚Üê Explicit is safer
    streaming_latency=3,
)
```

**Lesson:** When in doubt, explicit configuration > environment variable reliance, especially after git operations.

---

##  Process Management Challenges

### 45. **Port 8081 Already in Use**

**Problem:**  
```
OSError: [Errno 48] error while attempting to bind on address ('::', 8081, 0, 0): [errno 48] address already in use
```

**Root Cause:**  
Zombie `python start_agent.py` processes from previous runs still holding port 8081.

**Solution:**  
Kill all agent processes before restarting:
```bash
# Find process holding port
lsof -i :8081

# Kill by PID
kill -9 <PID>

# Or kill all matching processes
pkill -9 -f "python start_agent.py"
```

**Better Solution:** Use process supervisor (systemd, PM2, or supervisor) for automatic cleanup.

---

##  Key Learnings

### What Worked Well
 RAG with reranking dramatically reduced hallucinations  
 Parallel async execution cut latency by 60%+  
 Direct PostgreSQL access solved RLS headaches  
 Context managers prevented connection leaks  
 Caching calendar data improved UX significantly  
 Heuristic-based filtering for book ingestion cleaned RAG results  
 Explicit API parameter passing > environment variable reliance  

### What We'd Do Differently
 Start with simpler architecture (no Neo4j from day 1)  
 Add comprehensive logging earlier in development  
 Use feature flags for expensive operations (embedding, LLM calls)  
 Implement circuit breakers for external APIs  
 Add monitoring/observability from start (not retrofit)  
 Plan for data cleanup scripts when building RAG ingestion pipelines  
 Update all call sites when refactoring method signatures  

### Tools That Saved Us
 `asyncio.gather()` for parallelization  
 `asyncio.wait_for()` for timeouts  
 Context managers (`@contextmanager`) for cleanup  
 LiveKit's plugin ecosystem  
 Voyage AI's reranker for precision  
 `lsof` and `pkill` for process debugging  

---

##  Future Improvements

1. **Circuit Breakers** - Prevent cascading failures from external APIs
2. **Distributed Tracing** - OpenTelemetry for request lifecycle visibility
3. **A/B Testing** - Compare prompt variations, RAG strategies
4. **Real-time Monitoring** - Prometheus + Grafana for metrics
5. **Error Recovery** - Automatic retry with exponential backoff
6. **Feature Flags** - LaunchDarkly for gradual rollouts
7. **Load Testing** - Identify breaking points before production
8. **Process Supervisor** - systemd/PM2 for automatic agent restart
9. **Data Versioning** - Track RAG ingestion pipeline iterations

---

**Last Updated:** Nov 28, 2024  
**Total Challenges Documented:** 45  
**Estimated Debugging Hours:** 140+  

*"Every bug is a lesson, every crash is an opportunity to build more resilient systems."*

---

### 35. **Fly.io Deployment Crash (OpenTelemetry Mismatch)**

**Problem:**  
Deployment to Fly.io failed with `ImportError: cannot import name 'LogData' from 'opentelemetry.sdk._logs'`.

**Root Cause:**  
Dependency version mismatch. `livekit-agents` required a specific version of OpenTelemetry, but `pip install` pulled a newer, incompatible version because `requirements.txt` didn't pin versions.

**Solution:**  
Pinned exact versions in `requirements.txt` matching the working local environment:
```text
opentelemetry-api==1.38.0
opentelemetry-sdk==1.38.0
opentelemetry-exporter-otlp==1.38.0
opentelemetry-instrumentation==0.59b0
```

---

### 36. **Multiple Agents Joining Same Room**

**Problem:**  
Three agent instances would join a single room, causing chaos.

**Root Cause:**  
1. **Double Dispatch:** Code had a duplicate `create_dispatch` call (copy-paste error).
2. **Frontend + Backend Race:** Frontend called `dispatch-agent` endpoint while backend *also* had auto-join logic.

**Solution:**  
1. Removed duplicate line in `main.py`.
2. Added check for existing dispatches before creating new one:
```python
existing = await lkapi.agent_dispatch.list_dispatch(room_name=room_name)
if existing:
    return existing[0]
```

---

### 37. **Database Connection Exhaustion (Fly.io)**

**Problem:**  
Potential for "too many clients" errors on Fly.io.

**Root Cause:**  
Application opens a new DB connection for every request. Direct Postgres connection (port 5432) has a limit of ~100 connections.

**Solution:**  
Switched to **Supabase Transaction Pooler** (port 6543). This allows thousands of short-lived connections to share a small pool of real connections, perfectly matching the app's connection pattern.
